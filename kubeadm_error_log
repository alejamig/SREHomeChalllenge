fatal: [k8s-master]: FAILED! => {"changed": true, "cmd": ["sudo", "kubeadm", "init", "--apiserver-advertise-address=192.168.50.10", "--apiserver-cert-extra-sans=192.168.50.10", "--node-name", "k8s-master", "--pod-network-cidr=192.168.0.0/16", "--v=5"], "delta": "0:03:34.198073", "end": "2021-09-13 02:03:37.104255", "msg": "non-zero return code", "rc": 1, "start": "2021-09-13 02:00:02.906182", "stderr": "I0913 02:00:03.062867    6455 initconfiguration.go:116] detected and using CRI socket: /var/run/dockershim.sock\nI0913 02:00:03.063048    6455 kubelet.go:203] the value of KubeletConfiguration.cgroupDriver is empty; setting it to \"systemd\"\nI0913 02:00:03.075111    6455 version.go:186] fetching Kubernetes version from URL: https://dl.k8s.io/release/stable-1.txt\nI0913 02:00:04.199892    6455 checks.go:577] validating Kubernetes and kubeadm version\nI0913 02:00:04.199937    6455 checks.go:170] validating if the firewall is enabled and active\nI0913 02:00:04.224304    6455 checks.go:205] validating availability of port 6443\nI0913 02:00:04.225025    6455 checks.go:205] validating availability of port 10259\nI0913 02:00:04.225143    6455 checks.go:205] validating availability of port 10257\nI0913 02:00:04.225175    6455 checks.go:282] validating the existence of file /etc/kubernetes/manifests/kube-apiserver.yaml\nI0913 02:00:04.225211    6455 checks.go:282] validating the existence of file /etc/kubernetes/manifests/kube-controller-manager.yaml\nI0913 02:00:04.225223    6455 checks.go:282] validating the existence of file /etc/kubernetes/manifests/kube-scheduler.yaml\nI0913 02:00:04.225232    6455 checks.go:282] validating the existence of file /etc/kubernetes/manifests/etcd.yaml\nI0913 02:00:04.225243    6455 checks.go:432] validating if the connectivity type is via proxy or direct\nI0913 02:00:04.225807    6455 checks.go:471] validating http connectivity to first IP address in the CIDR\nI0913 02:00:04.225932    6455 checks.go:471] validating http connectivity to first IP address in the CIDR\nI0913 02:00:04.225953    6455 checks.go:106] validating the container runtime\nI0913 02:00:04.753728    6455 checks.go:132] validating if the \"docker\" service is enabled and active\nI0913 02:00:04.800708    6455 checks.go:331] validating the contents of file /proc/sys/net/bridge/bridge-nf-call-iptables\nI0913 02:00:04.800784    6455 checks.go:331] validating the contents of file /proc/sys/net/ipv4/ip_forward\nI0913 02:00:04.800816    6455 checks.go:649] validating whether swap is enabled or not\nI0913 02:00:04.800855    6455 checks.go:372] validating the presence of executable conntrack\nI0913 02:00:04.800905    6455 checks.go:372] validating the presence of executable ip\nI0913 02:00:04.800924    6455 checks.go:372] validating the presence of executable iptables\nI0913 02:00:04.800942    6455 checks.go:372] validating the presence of executable mount\nI0913 02:00:04.800962    6455 checks.go:372] validating the presence of executable nsenter\nI0913 02:00:04.801007    6455 checks.go:372] validating the presence of executable ebtables\nI0913 02:00:04.801047    6455 checks.go:372] validating the presence of executable ethtool\nI0913 02:00:04.801061    6455 checks.go:372] validating the presence of executable socat\nI0913 02:00:04.801085    6455 checks.go:372] validating the presence of executable tc\nI0913 02:00:04.801111    6455 checks.go:372] validating the presence of executable touch\nI0913 02:00:04.801130    6455 checks.go:520] running all checks\nI0913 02:00:05.220668    6455 checks.go:403] checking whether the given node name is valid and reachable using net.LookupHost\nI0913 02:00:05.220703    6455 checks.go:618] validating kubelet version\nI0913 02:00:05.418376    6455 checks.go:132] validating if the \"kubelet\" service is enabled and active\nI0913 02:00:05.452227    6455 checks.go:205] validating availability of port 10250\nI0913 02:00:05.452334    6455 checks.go:205] validating availability of port 2379\nI0913 02:00:05.452363    6455 checks.go:205] validating availability of port 2380\nI0913 02:00:05.452818    6455 checks.go:245] validating the existence and emptiness of directory /var/lib/etcd\nI0913 02:00:05.453671    6455 checks.go:838] using image pull policy: IfNotPresent\nI0913 02:00:05.640961    6455 checks.go:855] pulling: k8s.gcr.io/kube-apiserver:v1.22.1\nI0913 02:00:17.529079    6455 checks.go:855] pulling: k8s.gcr.io/kube-controller-manager:v1.22.1\nI0913 02:00:25.900551    6455 checks.go:855] pulling: k8s.gcr.io/kube-scheduler:v1.22.1\nI0913 02:00:32.369432    6455 checks.go:855] pulling: k8s.gcr.io/kube-proxy:v1.22.1\nI0913 02:00:48.043696    6455 checks.go:855] pulling: k8s.gcr.io/pause:3.5\nI0913 02:00:49.965920    6455 checks.go:855] pulling: k8s.gcr.io/etcd:3.5.0-0\nI0913 02:01:24.758802    6455 checks.go:855] pulling: k8s.gcr.io/coredns/coredns:v1.8.4\nI0913 02:01:31.129714    6455 certs.go:111] creating a new certificate authority for ca\nI0913 02:01:31.428208    6455 certs.go:487] validating certificate period for ca certificate\nI0913 02:01:32.918735    6455 certs.go:111] creating a new certificate authority for front-proxy-ca\nI0913 02:01:33.435453    6455 certs.go:487] validating certificate period for front-proxy-ca certificate\nI0913 02:01:33.866295    6455 certs.go:111] creating a new certificate authority for etcd-ca\nI0913 02:01:35.031620    6455 certs.go:487] validating certificate period for etcd/ca certificate\nI0913 02:01:37.310076    6455 certs.go:77] creating new public/private key files for signing service account users\nI0913 02:01:37.752578    6455 kubeconfig.go:103] creating kubeconfig file for admin.conf\nI0913 02:01:38.820680    6455 kubeconfig.go:103] creating kubeconfig file for kubelet.conf\nI0913 02:01:39.318126    6455 kubeconfig.go:103] creating kubeconfig file for controller-manager.conf\nI0913 02:01:39.660621    6455 kubeconfig.go:103] creating kubeconfig file for scheduler.conf\nI0913 02:01:41.349011    6455 kubelet.go:65] Stopping the kubelet\nI0913 02:01:41.970586    6455 manifests.go:99] [control-plane] getting StaticPodSpecs\nI0913 02:01:41.971085    6455 certs.go:487] validating certificate period for CA certificate\nI0913 02:01:41.971230    6455 manifests.go:125] [control-plane] adding volume \"ca-certs\" for component \"kube-apiserver\"\nI0913 02:01:41.971239    6455 manifests.go:125] [control-plane] adding volume \"etc-ca-certificates\" for component \"kube-apiserver\"\nI0913 02:01:41.971247    6455 manifests.go:125] [control-plane] adding volume \"etc-pki\" for component \"kube-apiserver\"\nI0913 02:01:41.971253    6455 manifests.go:125] [control-plane] adding volume \"k8s-certs\" for component \"kube-apiserver\"\nI0913 02:01:41.971261    6455 manifests.go:125] [control-plane] adding volume \"usr-local-share-ca-certificates\" for component \"kube-apiserver\"\nI0913 02:01:41.971267    6455 manifests.go:125] [control-plane] adding volume \"usr-share-ca-certificates\" for component \"kube-apiserver\"\nI0913 02:01:42.019114    6455 manifests.go:154] [control-plane] wrote static Pod manifest for component \"kube-apiserver\" to \"/etc/kubernetes/manifests/kube-apiserver.yaml\"\nI0913 02:01:42.019310    6455 manifests.go:99] [control-plane] getting StaticPodSpecs\nI0913 02:01:42.019841    6455 manifests.go:125] [control-plane] adding volume \"ca-certs\" for component \"kube-controller-manager\"\nI0913 02:01:42.019975    6455 manifests.go:125] [control-plane] adding volume \"etc-ca-certificates\" for component \"kube-controller-manager\"\nI0913 02:01:42.019990    6455 manifests.go:125] [control-plane] adding volume \"etc-pki\" for component \"kube-controller-manager\"\nI0913 02:01:42.019997    6455 manifests.go:125] [control-plane] adding volume \"flexvolume-dir\" for component \"kube-controller-manager\"\nI0913 02:01:42.020004    6455 manifests.go:125] [control-plane] adding volume \"k8s-certs\" for component \"kube-controller-manager\"\nI0913 02:01:42.020009    6455 manifests.go:125] [control-plane] adding volume \"kubeconfig\" for component \"kube-controller-manager\"\nI0913 02:01:42.020016    6455 manifests.go:125] [control-plane] adding volume \"usr-local-share-ca-certificates\" for component \"kube-controller-manager\"\nI0913 02:01:42.020023    6455 manifests.go:125] [control-plane] adding volume \"usr-share-ca-certificates\" for component \"kube-controller-manager\"\nI0913 02:01:42.021990    6455 manifests.go:154] [control-plane] wrote static Pod manifest for component \"kube-controller-manager\" to \"/etc/kubernetes/manifests/kube-controller-manager.yaml\"\nI0913 02:01:42.022107    6455 manifests.go:99] [control-plane] getting StaticPodSpecs\nI0913 02:01:42.022746    6455 manifests.go:125] [control-plane] adding volume \"kubeconfig\" for component \"kube-scheduler\"\nI0913 02:01:42.037330    6455 manifests.go:154] [control-plane] wrote static Pod manifest for component \"kube-scheduler\" to \"/etc/kubernetes/manifests/kube-scheduler.yaml\"\nI0913 02:01:42.040242    6455 local.go:65] [etcd] wrote Static Pod manifest for a local etcd member to \"/etc/kubernetes/manifests/etcd.yaml\"\nI0913 02:01:42.040362    6455 waitcontrolplane.go:89] [wait-control-plane] Waiting for the API server to be healthy\ncouldn't initialize a Kubernetes cluster\nk8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/init.runWaitControlPlanePhase\n\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/init/waitcontrolplane.go:116\nk8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).Run.func1\n\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:234\nk8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).visitAll\n\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:421\nk8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).Run\n\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:207\nk8s.io/kubernetes/cmd/kubeadm/app/cmd.newCmdInit.func1\n\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/init.go:153\nk8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).execute\n\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:852\nk8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).ExecuteC\n\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:960\nk8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).Execute\n\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:897\nk8s.io/kubernetes/cmd/kubeadm/app.Run\n\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/kubeadm.go:50\nmain.main\n\t_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/kubeadm.go:25\nruntime.main\n\t/usr/local/go/src/runtime/proc.go:225\nruntime.goexit\n\t/usr/local/go/src/runtime/asm_amd64.s:1371\nerror execution phase wait-control-plane\nk8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).Run.func1\n\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:235\nk8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).visitAll\n\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:421\nk8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).Run\n\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:207\nk8s.io/kubernetes/cmd/kubeadm/app/cmd.newCmdInit.func1\n\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/init.go:153\nk8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).execute\n\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:852\nk8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).ExecuteC\n\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:960\nk8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).Execute\n\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:897\nk8s.io/kubernetes/cmd/kubeadm/app.Run\n\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/kubeadm.go:50\nmain.main\n\t_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/kubeadm.go:25\nruntime.main\n\t/usr/local/go/src/runtime/proc.go:225\nruntime.goexit\n\t/usr/local/go/src/runtime/asm_amd64.s:1371", "stderr_lines": ["I0913 02:00:03.062867    6455 initconfiguration.go:116] detected and using CRI socket: /var/run/dockershim.sock", "I0913 02:00:03.063048    6455 kubelet.go:203] the value of KubeletConfiguration.cgroupDriver is empty; setting it to \"systemd\"", "I0913 02:00:03.075111    6455 version.go:186] fetching Kubernetes version from URL: https://dl.k8s.io/release/stable-1.txt", "I0913 02:00:04.199892    6455 checks.go:577] validating Kubernetes and kubeadm version", "I0913 02:00:04.199937    6455 checks.go:170] validating if the firewall is enabled and active", "I0913 02:00:04.224304    6455 checks.go:205] validating availability of port 6443", "I0913 02:00:04.225025    6455 checks.go:205] validating availability of port 10259", "I0913 02:00:04.225143    6455 checks.go:205] validating availability of port 10257", "I0913 02:00:04.225175    6455 checks.go:282] validating the existence of file /etc/kubernetes/manifests/kube-apiserver.yaml", "I0913 02:00:04.225211    6455 checks.go:282] validating the existence of file /etc/kubernetes/manifests/kube-controller-manager.yaml", "I0913 02:00:04.225223    6455 checks.go:282] validating the existence of file /etc/kubernetes/manifests/kube-scheduler.yaml", "I0913 02:00:04.225232    6455 checks.go:282] validating the existence of file /etc/kubernetes/manifests/etcd.yaml", "I0913 02:00:04.225243    6455 checks.go:432] validating if the connectivity type is via proxy or direct", "I0913 02:00:04.225807    6455 checks.go:471] validating http connectivity to first IP address in the CIDR", "I0913 02:00:04.225932    6455 checks.go:471] validating http connectivity to first IP address in the CIDR", "I0913 02:00:04.225953    6455 checks.go:106] validating the container runtime", "I0913 02:00:04.753728    6455 checks.go:132] validating if the \"docker\" service is enabled and active", "I0913 02:00:04.800708    6455 checks.go:331] validating the contents of file /proc/sys/net/bridge/bridge-nf-call-iptables", "I0913 02:00:04.800784    6455 checks.go:331] validating the contents of file /proc/sys/net/ipv4/ip_forward", "I0913 02:00:04.800816    6455 checks.go:649] validating whether swap is enabled or not", "I0913 02:00:04.800855    6455 checks.go:372] validating the presence of executable conntrack", "I0913 02:00:04.800905    6455 checks.go:372] validating the presence of executable ip", "I0913 02:00:04.800924    6455 checks.go:372] validating the presence of executable iptables", "I0913 02:00:04.800942    6455 checks.go:372] validating the presence of executable mount", "I0913 02:00:04.800962    6455 checks.go:372] validating the presence of executable nsenter", "I0913 02:00:04.801007    6455 checks.go:372] validating the presence of executable ebtables", "I0913 02:00:04.801047    6455 checks.go:372] validating the presence of executable ethtool", "I0913 02:00:04.801061    6455 checks.go:372] validating the presence of executable socat", "I0913 02:00:04.801085    6455 checks.go:372] validating the presence of executable tc", "I0913 02:00:04.801111    6455 checks.go:372] validating the presence of executable touch", "I0913 02:00:04.801130    6455 checks.go:520] running all checks", "I0913 02:00:05.220668    6455 checks.go:403] checking whether the given node name is valid and reachable using net.LookupHost", "I0913 02:00:05.220703    6455 checks.go:618] validating kubelet version", "I0913 02:00:05.418376    6455 checks.go:132] validating if the \"kubelet\" service is enabled and active", "I0913 02:00:05.452227    6455 checks.go:205] validating availability of port 10250", "I0913 02:00:05.452334    6455 checks.go:205] validating availability of port 2379", "I0913 02:00:05.452363    6455 checks.go:205] validating availability of port 2380", "I0913 02:00:05.452818    6455 checks.go:245] validating the existence and emptiness of directory /var/lib/etcd", "I0913 02:00:05.453671    6455 checks.go:838] using image pull policy: IfNotPresent", "I0913 02:00:05.640961    6455 checks.go:855] pulling: k8s.gcr.io/kube-apiserver:v1.22.1", "I0913 02:00:17.529079    6455 checks.go:855] pulling: k8s.gcr.io/kube-controller-manager:v1.22.1", "I0913 02:00:25.900551    6455 checks.go:855] pulling: k8s.gcr.io/kube-scheduler:v1.22.1", "I0913 02:00:32.369432    6455 checks.go:855] pulling: k8s.gcr.io/kube-proxy:v1.22.1", "I0913 02:00:48.043696    6455 checks.go:855] pulling: k8s.gcr.io/pause:3.5", "I0913 02:00:49.965920    6455 checks.go:855] pulling: k8s.gcr.io/etcd:3.5.0-0", "I0913 02:01:24.758802    6455 checks.go:855] pulling: k8s.gcr.io/coredns/coredns:v1.8.4", "I0913 02:01:31.129714    6455 certs.go:111] creating a new certificate authority for ca", "I0913 02:01:31.428208    6455 certs.go:487] validating certificate period for ca certificate", "I0913 02:01:32.918735    6455 certs.go:111] creating a new certificate authority for front-proxy-ca", "I0913 02:01:33.435453    6455 certs.go:487] validating certificate period for front-proxy-ca certificate", "I0913 02:01:33.866295    6455 certs.go:111] creating a new certificate authority for etcd-ca", "I0913 02:01:35.031620    6455 certs.go:487] validating certificate period for etcd/ca certificate", "I0913 02:01:37.310076    6455 certs.go:77] creating new public/private key files for signing service account users", "I0913 02:01:37.752578    6455 kubeconfig.go:103] creating kubeconfig file for admin.conf", "I0913 02:01:38.820680    6455 kubeconfig.go:103] creating kubeconfig file for kubelet.conf", "I0913 02:01:39.318126    6455 kubeconfig.go:103] creating kubeconfig file for controller-manager.conf", "I0913 02:01:39.660621    6455 kubeconfig.go:103] creating kubeconfig file for scheduler.conf", "I0913 02:01:41.349011    6455 kubelet.go:65] Stopping the kubelet", "I0913 02:01:41.970586    6455 manifests.go:99] [control-plane] getting StaticPodSpecs", "I0913 02:01:41.971085    6455 certs.go:487] validating certificate period for CA certificate", "I0913 02:01:41.971230    6455 manifests.go:125] [control-plane] adding volume \"ca-certs\" for component \"kube-apiserver\"", "I0913 02:01:41.971239    6455 manifests.go:125] [control-plane] adding volume \"etc-ca-certificates\" for component \"kube-apiserver\"", "I0913 02:01:41.971247    6455 manifests.go:125] [control-plane] adding volume \"etc-pki\" for component \"kube-apiserver\"", "I0913 02:01:41.971253    6455 manifests.go:125] [control-plane] adding volume \"k8s-certs\" for component \"kube-apiserver\"", "I0913 02:01:41.971261    6455 manifests.go:125] [control-plane] adding volume \"usr-local-share-ca-certificates\" for component \"kube-apiserver\"", "I0913 02:01:41.971267    6455 manifests.go:125] [control-plane] adding volume \"usr-share-ca-certificates\" for component \"kube-apiserver\"", "I0913 02:01:42.019114    6455 manifests.go:154] [control-plane] wrote static Pod manifest for component \"kube-apiserver\" to \"/etc/kubernetes/manifests/kube-apiserver.yaml\"", "I0913 02:01:42.019310    6455 manifests.go:99] [control-plane] getting StaticPodSpecs", "I0913 02:01:42.019841    6455 manifests.go:125] [control-plane] adding volume \"ca-certs\" for component \"kube-controller-manager\"", "I0913 02:01:42.019975    6455 manifests.go:125] [control-plane] adding volume \"etc-ca-certificates\" for component \"kube-controller-manager\"", "I0913 02:01:42.019990    6455 manifests.go:125] [control-plane] adding volume \"etc-pki\" for component \"kube-controller-manager\"", "I0913 02:01:42.019997    6455 manifests.go:125] [control-plane] adding volume \"flexvolume-dir\" for component \"kube-controller-manager\"", "I0913 02:01:42.020004    6455 manifests.go:125] [control-plane] adding volume \"k8s-certs\" for component \"kube-controller-manager\"", "I0913 02:01:42.020009    6455 manifests.go:125] [control-plane] adding volume \"kubeconfig\" for component \"kube-controller-manager\"", "I0913 02:01:42.020016    6455 manifests.go:125] [control-plane] adding volume \"usr-local-share-ca-certificates\" for component \"kube-controller-manager\"", "I0913 02:01:42.020023    6455 manifests.go:125] [control-plane] adding volume \"usr-share-ca-certificates\" for component \"kube-controller-manager\"", "I0913 02:01:42.021990    6455 manifests.go:154] [control-plane] wrote static Pod manifest for component \"kube-controller-manager\" to \"/etc/kubernetes/manifests/kube-controller-manager.yaml\"", "I0913 02:01:42.022107    6455 manifests.go:99] [control-plane] getting StaticPodSpecs", "I0913 02:01:42.022746    6455 manifests.go:125] [control-plane] adding volume \"kubeconfig\" for component \"kube-scheduler\"", "I0913 02:01:42.037330    6455 manifests.go:154] [control-plane] wrote static Pod manifest for component \"kube-scheduler\" to \"/etc/kubernetes/manifests/kube-scheduler.yaml\"", "I0913 02:01:42.040242    6455 local.go:65] [etcd] wrote Static Pod manifest for a local etcd member to \"/etc/kubernetes/manifests/etcd.yaml\"", "I0913 02:01:42.040362    6455 waitcontrolplane.go:89] [wait-control-plane] Waiting for the API server to be healthy", "couldn't initialize a Kubernetes cluster", "k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/init.runWaitControlPlanePhase", "\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/init/waitcontrolplane.go:116", "k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).Run.func1", "\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:234", "k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).visitAll", "\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:421", "k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).Run", "\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:207", "k8s.io/kubernetes/cmd/kubeadm/app/cmd.newCmdInit.func1", "\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/init.go:153", "k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).execute", "\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:852", "k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).ExecuteC", "\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:960", "k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).Execute", "\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:897", "k8s.io/kubernetes/cmd/kubeadm/app.Run", "\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/kubeadm.go:50", "main.main", "\t_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/kubeadm.go:25", "runtime.main", "\t/usr/local/go/src/runtime/proc.go:225", "runtime.goexit", "\t/usr/local/go/src/runtime/asm_amd64.s:1371", "error execution phase wait-control-plane", "k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).Run.func1", "\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:235", "k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).visitAll", "\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:421", "k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).Run", "\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:207", "k8s.io/kubernetes/cmd/kubeadm/app/cmd.newCmdInit.func1", "\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/init.go:153", "k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).execute", "\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:852", "k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).ExecuteC", "\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:960", "k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).Execute", "\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:897", "k8s.io/kubernetes/cmd/kubeadm/app.Run", "\t/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/kubeadm.go:50", "main.main", "\t_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/kubeadm.go:25", "runtime.main", "\t/usr/local/go/src/runtime/proc.go:225", "runtime.goexit", "\t/usr/local/go/src/runtime/asm_amd64.s:1371"], "stdout": "[init] Using Kubernetes version: v1.22.1\n[preflight] Running pre-flight checks\n[preflight] Pulling images required for setting up a Kubernetes cluster\n[preflight] This might take a minute or two, depending on the speed of your internet connection\n[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'\n[certs] Using certificateDir folder \"/etc/kubernetes/pki\"\n[certs] Generating \"ca\" certificate and key\n[certs] Generating \"apiserver\" certificate and key\n[certs] apiserver serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.50.10]\n[certs] Generating \"apiserver-kubelet-client\" certificate and key\n[certs] Generating \"front-proxy-ca\" certificate and key\n[certs] Generating \"front-proxy-client\" certificate and key\n[certs] Generating \"etcd/ca\" certificate and key\n[certs] Generating \"etcd/server\" certificate and key\n[certs] etcd/server serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.50.10 127.0.0.1 ::1]\n[certs] Generating \"etcd/peer\" certificate and key\n[certs] etcd/peer serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.50.10 127.0.0.1 ::1]\n[certs] Generating \"etcd/healthcheck-client\" certificate and key\n[certs] Generating \"apiserver-etcd-client\" certificate and key\n[certs] Generating \"sa\" key and public key\n[kubeconfig] Using kubeconfig folder \"/etc/kubernetes\"\n[kubeconfig] Writing \"admin.conf\" kubeconfig file\n[kubeconfig] Writing \"kubelet.conf\" kubeconfig file\n[kubeconfig] Writing \"controller-manager.conf\" kubeconfig file\n[kubeconfig] Writing \"scheduler.conf\" kubeconfig file\n[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n[kubelet-start] Starting the kubelet\n[control-plane] Using manifest folder \"/etc/kubernetes/manifests\"\n[control-plane] Creating static Pod manifest for \"kube-apiserver\"\n[control-plane] Creating static Pod manifest for \"kube-controller-manager\"\n[control-plane] Creating static Pod manifest for \"kube-scheduler\"\n[etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\"\n[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s\n[kubelet-check] Initial timeout of 40s passed.\n[kubelet-check] It seems like the kubelet isn't running or healthy.\n[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get \"http://localhost:10248/healthz\": dial tcp [::1]:10248: connect: connection refused.\n[kubelet-check] It seems like the kubelet isn't running or healthy.\n[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get \"http://localhost:10248/healthz\": dial tcp [::1]:10248: connect: connection refused.\n[kubelet-check] It seems like the kubelet isn't running or healthy.\n[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get \"http://localhost:10248/healthz\": dial tcp [::1]:10248: connect: connection refused.\n[kubelet-check] It seems like the kubelet isn't running or healthy.\n[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get \"http://localhost:10248/healthz\": dial tcp [::1]:10248: connect: connection refused.\n[kubelet-check] It seems like the kubelet isn't running or healthy.\n[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get \"http://localhost:10248/healthz\": dial tcp [::1]:10248: connect: connection refused.\n\n\tUnfortunately, an error has occurred:\n\t\ttimed out waiting for the condition\n\n\tThis error is likely caused by:\n\t\t- The kubelet is not running\n\t\t- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)\n\n\tIf you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:\n\t\t- 'systemctl status kubelet'\n\t\t- 'journalctl -xeu kubelet'\n\n\tAdditionally, a control plane component may have crashed or exited when started by the container runtime.\n\tTo troubleshoot, list all containers using your preferred container runtimes CLI.\n\n\tHere is one example how you may list all Kubernetes containers running in docker:\n\t\t- 'docker ps -a | grep kube | grep -v pause'\n\t\tOnce you have found the failing container, you can inspect its logs with:\n\t\t- 'docker logs CONTAINERID'", "stdout_lines": ["[init] Using Kubernetes version: v1.22.1", "[preflight] Running pre-flight checks", "[preflight] Pulling images required for setting up a Kubernetes cluster", "[preflight] This might take a minute or two, depending on the speed of your internet connection", "[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'", "[certs] Using certificateDir folder \"/etc/kubernetes/pki\"", "[certs] Generating \"ca\" certificate and key", "[certs] Generating \"apiserver\" certificate and key", "[certs] apiserver serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.50.10]", "[certs] Generating \"apiserver-kubelet-client\" certificate and key", "[certs] Generating \"front-proxy-ca\" certificate and key", "[certs] Generating \"front-proxy-client\" certificate and key", "[certs] Generating \"etcd/ca\" certificate and key", "[certs] Generating \"etcd/server\" certificate and key", "[certs] etcd/server serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.50.10 127.0.0.1 ::1]", "[certs] Generating \"etcd/peer\" certificate and key", "[certs] etcd/peer serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.50.10 127.0.0.1 ::1]", "[certs] Generating \"etcd/healthcheck-client\" certificate and key", "[certs] Generating \"apiserver-etcd-client\" certificate and key", "[certs] Generating \"sa\" key and public key", "[kubeconfig] Using kubeconfig folder \"/etc/kubernetes\"", "[kubeconfig] Writing \"admin.conf\" kubeconfig file", "[kubeconfig] Writing \"kubelet.conf\" kubeconfig file", "[kubeconfig] Writing \"controller-manager.conf\" kubeconfig file", "[kubeconfig] Writing \"scheduler.conf\" kubeconfig file", "[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"", "[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"", "[kubelet-start] Starting the kubelet", "[control-plane] Using manifest folder \"/etc/kubernetes/manifests\"", "[control-plane] Creating static Pod manifest for \"kube-apiserver\"", "[control-plane] Creating static Pod manifest for \"kube-controller-manager\"", "[control-plane] Creating static Pod manifest for \"kube-scheduler\"", "[etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\"", "[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s", "[kubelet-check] Initial timeout of 40s passed.", "[kubelet-check] It seems like the kubelet isn't running or healthy.", "[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get \"http://localhost:10248/healthz\": dial tcp [::1]:10248: connect: connection refused.", "[kubelet-check] It seems like the kubelet isn't running or healthy.", "[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get \"http://localhost:10248/healthz\": dial tcp [::1]:10248: connect: connection refused.", "[kubelet-check] It seems like the kubelet isn't running or healthy.", "[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get \"http://localhost:10248/healthz\": dial tcp [::1]:10248: connect: connection refused.", "[kubelet-check] It seems like the kubelet isn't running or healthy.", "[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get \"http://localhost:10248/healthz\": dial tcp [::1]:10248: connect: connection refused.", "[kubelet-check] It seems like the kubelet isn't running or healthy.", "[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get \"http://localhost:10248/healthz\": dial tcp [::1]:10248: connect: connection refused.", "", "\tUnfortunately, an error has occurred:", "\t\ttimed out waiting for the condition", "", "\tThis error is likely caused by:", "\t\t- The kubelet is not running", "\t\t- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)", "", "\tIf you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:", "\t\t- 'systemctl status kubelet'", "\t\t- 'journalctl -xeu kubelet'", "", "\tAdditionally, a control plane component may have crashed or exited when started by the container runtime.", "\tTo troubleshoot, list all containers using your preferred container runtimes CLI.", "", "\tHere is one example how you may list all Kubernetes containers running in docker:", "\t\t- 'docker ps -a | grep kube | grep -v pause'", "\t\tOnce you have found the failing container, you can inspect its logs with:", "\t\t- 'docker logs CONTAINERID'"]}
